{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class value:\n",
    "    def __init__(self,data, _children=(), _oper ='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._oper = _oper\n",
    "        self.label = label\n",
    "    def __repr__(self):\n",
    "        return f\"value(data={self.data})\"\n",
    "        \n",
    "    def __add__(self,other):\n",
    "        other = other if isinstance(other, value) else value(other)\n",
    "        out = value(self.data + other.data, (self,other), '+')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += 1*out.grad\n",
    "            other.grad += 1*out.grad\n",
    "        out._backward = _backward    \n",
    "\n",
    "        return out\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        out = value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "  \n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other): # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "       \n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, value) else value(other)\n",
    "        out = value(self.data * other.data, (self,other), '*' )\n",
    "        def _backward():\n",
    "            self.grad += other.data*out.grad\n",
    "            other.grad+= self.data*out.grad\n",
    "        out._backward = _backward    \n",
    "\n",
    "        return out \n",
    "        \n",
    "    \n",
    "    def tanh(self):\n",
    "        x= self.data\n",
    "        t = ((math.exp(2*x)-1)/(math.exp(2*x)+1))\n",
    "        out = value(t, (self, ), 'tanh')\n",
    "        def _backward():\n",
    "            self.grad = (1-t**2)*out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "     \n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = value(math.exp(x), (self, ), 'exp')\n",
    "    \n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad # NOTE: in the video I incorrectly used = instead of +=. Fixed here.\n",
    "        out._backward = _backward\n",
    "    \n",
    "        return out \n",
    "    \n",
    "    def backward(self):\n",
    "     topo = []\n",
    "     visited = set()\n",
    "     def build_topo(v):\n",
    "       if v not in visited:\n",
    "         visited.add(v)\n",
    "         for child in v._prev:\n",
    "           build_topo(child)\n",
    "         topo.append(v)\n",
    "     build_topo(self)\n",
    "    \n",
    "     self.grad = 1.0\n",
    "     for node in reversed(topo):\n",
    "       node._backward()   \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "  # builds a set of all nodes and edges in a graph\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v._prev:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "  \n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # for any value in the graph, create a rectangular ('record') node for it\n",
    "    dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
    "    if n._oper:\n",
    "      # if this value is a result of some operation, create an op node for it\n",
    "      dot.node(name = uid + n._oper, label = n._oper)\n",
    "      # and connect this node to it\n",
    "      dot.edge(uid + n._oper, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # connect n1 to the op node of n2\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._oper)\n",
    "\n",
    "  return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071066904050358\n",
      "---\n",
      "x2 0.5000001283844369\n",
      "w2 0.0\n",
      "x1 -1.5000003851533106\n",
      "w1 1.0000002567688737\n"
     ]
    }
   ],
   "source": [
    "#test case\n",
    "x1 = torch.Tensor([2.0]).double()                ; x1.requires_grad = True    #bydeafult pytorch keeps grads off for scalars & x1 was typecasted to double to be consistent with python which uses float64\n",
    "x2 = torch.Tensor([0.0]).double()                ; x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double()               ; w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double()                ; w2.requires_grad = True\n",
    "b = torch.Tensor([6.8813735870195432]).double()  ; b.requires_grad = True\n",
    "n = x1*w1 + x2*w2 + b\n",
    "o = torch.tanh(n)\n",
    "\n",
    "print(o.data.item())\n",
    "o.backward()\n",
    "\n",
    "print('---')\n",
    "print('x2', x2.grad.item())\n",
    "print('w2', w2.grad.item())\n",
    "print('x1', x1.grad.item())\n",
    "print('w1', w1.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are tyring to build a neural network where we feed some data x's and then weight is multiplies to  to it, then a summation of wixi is done with the addition of a bias(activation function) and then passed thorugh tanh. For calculating the best w's we'll be implementing loss function on which we can perform optimisation which will directly effect the weights to give us better answer\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "class Neurons:\n",
    "\n",
    "    def __init__(self, nin):\n",
    "        self.w = [value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b = value(random.uniform(-1,1))\n",
    "    def __call__(self,x):\n",
    "        activation = sum((wi*xi for wi,xi in zip(self.w,x)), self.b)\n",
    "        out  = activation.tanh()\n",
    "        return out\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "#Now defining layers of mlp\n",
    "class Layers:\n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neurons(nin) for _ in range(nout)]\n",
    "    def __call__ (self, x):\n",
    "        outs= [n(x) for n in self.neurons]   \n",
    "        return outs[0] if len(outs) == 1 else outs \n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "class MLP:\n",
    "  \n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [layers(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "  \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    def parameters(self):\n",
    "        return[ p for layer in self.layers for p in layer.parameters() ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2.0, 3.0, -1.0]\n",
    "n = MLP(3, [4, 4, 1])\n",
    "n(x)\n",
    "len(n.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test case for the MLP\n",
    "\n",
    "xs = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.809928734170398e-05\n",
      "1 3.806026247876363e-05\n",
      "2 3.802131679097869e-05\n",
      "3 3.7982450038509405e-05\n",
      "4 3.7943661982492435e-05\n",
      "5 3.79049523850139e-05\n",
      "6 3.786632100912145e-05\n",
      "7 3.7827767618820554e-05\n",
      "8 3.778929197904946e-05\n",
      "9 3.775089385569891e-05\n",
      "10 3.771257301559196e-05\n",
      "11 3.767432922648368e-05\n",
      "12 3.76361622570598e-05\n",
      "13 3.759807187692828e-05\n",
      "14 3.756005785661362e-05\n"
     ]
    }
   ],
   "source": [
    "# defining loss function and applying gradient descent via forward-> backward->update step\n",
    "for k in range(15):\n",
    "    #forward pass\n",
    "    ypred = [n(x) for x in xs]\n",
    "    mse_loss = sum((yout-yreal)**2 for yout,yreal in zip(ypred, ys))\n",
    "\n",
    "#backward pass\n",
    "    for p in n.parameters():\n",
    "        p.grad = 0\n",
    "    mse_loss.backward()\n",
    "\n",
    "#update\n",
    "\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.6*p.grad  \n",
    "    print(k, mse_loss.data)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[value(data=0.9977838099248567),\n",
       " value(data=-0.9981817860291162),\n",
       " value(data=-0.9960119813578201),\n",
       " value(data=0.9963341624868035)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
